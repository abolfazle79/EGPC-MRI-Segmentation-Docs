---
sidebar_position: 2
title: Quantitative Evaluation
---

# Quantitative Evaluation

:::info Beyond Visuals: The Need for Metrics
While [visual results](./1-visual-results.md) provide an intuitive understanding of the algorithm's performance, a scientific evaluation requires objective, numerical metrics. Quantitative metrics allow for a direct and unbiased comparison between different methods and configurations. This page outlines the standard metrics used for evaluating the accuracy of a segmentation task.
:::

## The Prerequisite: Ground Truth

To perform a quantitative evaluation, we need a "correct" answer to compare our algorithm's output against. In medical image segmentation, this is known as the **Ground Truth mask**.

- **What is a Ground Truth?** A Ground Truth mask is a pixel-perfect delineation of the region of interest (in our case, the tumor), typically created by one or more expert radiologists.
- **Why is it needed?** By comparing the mask generated by our algorithm (`prediction`) with the ground truth mask (`actual`), we can count how many pixels were classified correctly and incorrectly.

## Core Statistical Concepts

The evaluation is based on comparing the prediction and the ground truth on a pixel-by-pixel basis. For each pixel, there are four possible outcomes:

- **True Positive (TP):** The pixel is part of the tumor, and the algorithm correctly identified it as tumor.
- **True Negative (TN):** The pixel is part of healthy tissue, and the algorithm correctly identified it as healthy tissue.
- **False Positive (FP):** The pixel is healthy tissue, but the algorithm incorrectly labeled it as tumor. (A "false alarm")
- **False Negative (FN):** The pixel is part of the tumor, but the algorithm missed it and labeled it as healthy tissue. (A "miss")

## Key Evaluation Metrics

Based on the counts of TP, TN, FP, and FN, we can calculate several standard metrics to measure performance.

### Dice Coefficient (F1-Score)

The Dice Coefficient is one of the most common and important metrics for segmentation tasks. It measures the overlap between the predicted segmentation and the ground truth. A value of 1 indicates a perfect match, while 0 indicates no overlap.

$$
\text{Dice Coefficient} = \frac{2 \times \text{TP}}{2 \times \text{TP} + \text{FP} + \text{FN}}
$$

### Accuracy

Accuracy measures the overall percentage of pixels that were correctly classified.

$$
\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
$$

:::caution
While intuitive, accuracy can be misleading for imbalanced datasets. For example, if a tumor occupies only 1% of the image, an algorithm that labels everything as "not tumor" would still have 99% accuracy, despite being completely useless.
:::

### Precision (Positive Predictive Value)

Precision answers the question: "Of all the pixels that the algorithm labeled as tumor, what percentage were actually tumor?"

$$
\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
$$

### Recall (Sensitivity)

Recall answers the question: "Of all the pixels that were actually tumor, what percentage did the algorithm successfully find?"

$$
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
$$

### Specificity

Specificity measures how well the algorithm correctly identifies non-tumor regions. It answers: "Of all the pixels that were actually healthy, what percentage did the algorithm correctly label as healthy?"

$$
\text{Specificity} = \frac{\text{TN}}{\text{TN} + \text{FP}}
$$

For a comprehensive evaluation in a research paper, presenting a combination of these metrics (especially the Dice Coefficient, Precision, and Recall) provides a robust and unbiased assessment of the algorithm's performance.
